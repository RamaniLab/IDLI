{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb0ddb5-9e21-4c6a-bcf2-9428604649c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm\n",
    "import boost_histogram as bh\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0683f271-4115-461c-87d7-93af9b5d6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_csv = pd.read_csv('/home/hrichter/IDLI_revisions/forZenodo/IDLI_Zenodo_SampleSheet.csv', sep=',')\n",
    "os.chdir('/home/hrichter/IDLI_revisions/forZenodo/random_nucs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c64ead-c55b-4c9b-9287-d91354e862cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array([0, 1])\n",
    "ref_sub = ref_csv[ref_csv.INDEX.isin(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbfeccd-cec9-4c26-97db-5a95e4e12f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eat_pickle(hmmFile):\n",
    "    with open(hmmFile, 'rb') as fin:\n",
    "        hmmdat = pickle.load(fin)\n",
    "    return hmmdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ec230-8c31-4f62-9a2a-cc2ee45c1cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E14_mESC_Rep37_Seq2_10k_sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████████████████████████▏                                                                                                                             | 2470/10000 [00:27<01:16, 98.85it/s]"
     ]
    }
   ],
   "source": [
    "#step 1: do IDLI! here for random nucs, up to 100,000 nucs per sample\n",
    "\n",
    "label = \"random_nucs\" #we use this label when just taking random nucs, rather than those near a specific region\n",
    "\n",
    "for idx, line in ref_sub.iterrows():\n",
    "    label_out = line['SAMPLE']\n",
    "    print(label_out)\n",
    "    frames = {}\n",
    "    \n",
    "    pickles_1 = line['HMMRES_1']\n",
    "    pickles_31 = line['HMMRES_31']\n",
    "    pickles_51 = line['HMMRES_51']\n",
    "    pickles_71 = line['HMMRES_71']\n",
    "    pickles_101 = line['HMMRES_101']\n",
    "\n",
    "    r0_acc = eat_pickle(line['HMMRES_1'])\n",
    "    r1_acc = eat_pickle(line['HMMRES_31'])\n",
    "    r2_acc = eat_pickle(line['HMMRES_51'])\n",
    "    r3_acc = eat_pickle(line['HMMRES_71'])\n",
    "    r4_acc = eat_pickle(line['HMMRES_101'])\n",
    "\n",
    "    foot_1 = line['FOOTPRINT_1']\n",
    "    foot_31 = line['FOOTPRINT_31']\n",
    "    foot_51 = line['FOOTPRINT_51']\n",
    "    foot_71 = line['FOOTPRINT_71']\n",
    "    foot_101 = line['FOOTPRINT_101']\n",
    "    \n",
    "    frames['1'] = pd.read_table(foot_1 ,sep=',',header=0)\n",
    "    frames['31'] = pd.read_table(foot_31 ,sep=',',header=0)\n",
    "    frames['51'] = pd.read_table(foot_51 ,sep=',',header=0)\n",
    "    frames['71'] = pd.read_table(foot_71 ,sep=',',header=0)\n",
    "    frames['101'] = pd.read_table(foot_101 ,sep=',',header=0)\n",
    "    \n",
    "    dim = (160,180)\n",
    "    vplots = {}\n",
    "    for key in frames:\n",
    "        vplots[key] = np.zeros(dim)\n",
    "    \n",
    "    ref_tot = frames['1']\n",
    "    r1_tot = frames['31']\n",
    "    r2_tot = frames['51']\n",
    "    r3_tot = frames['71']\n",
    "    r4_tot = frames['101']\n",
    "    \n",
    "    ref_frame = ref_tot[(ref_tot.length < 200) & (ref_tot.midpoint > 200)]\n",
    "    r0_frame = ref_frame[(ref_frame.length < 80) & (ref_frame.length > 20)]\n",
    "    r1_frame = r1_tot[(r1_tot.length < 200) & (r1_tot.length > 20)]\n",
    "    r2_frame = r2_tot[(r2_tot.length < 200) & (r2_tot.length > 20)]\n",
    "    r3_frame = r3_tot[(r3_tot.length < 200) & (r3_tot.length > 20)]\n",
    "    r4_frame = r4_tot[(r4_tot.length < 200) & (r4_tot.length > 20)]    \n",
    "    \n",
    "    zmw_ids = []\n",
    "    nuc_ids = []\n",
    "    label_ids = []\n",
    "    nuc_profiles_r0 = []\n",
    "    nuc_profiles_r1 = []\n",
    "    nuc_profiles_r2 = []\n",
    "    nuc_profiles_r3 = []\n",
    "    nuc_profiles_r4 = []\n",
    "\n",
    "    r0_mat = []\n",
    "    r1_mat = [] \n",
    "    r2_mat = []\n",
    "    r3_mat = []\n",
    "    r4_mat = []\n",
    "\n",
    "    nuc_acc_r0 = []\n",
    "    nuc_acc_r1 = []\n",
    "    nuc_acc_r2 = []\n",
    "    nuc_acc_r3 = []\n",
    "    nuc_acc_r4 = []\n",
    "\n",
    "    nuc_sizes = []\n",
    "    nuc_count = 0\n",
    "\n",
    "    zmws = list(r0_acc.keys())  # Extract keys from the r0_acc dictionary and convert to a list\n",
    "        \n",
    "    for i in tqdm(zmws):\n",
    "        ref = int(i)\n",
    "\n",
    "        #stops code at X number of nucleosomes - take how many you can computationally process, often 50-100k per sample\n",
    "        #comment out if you want ALL nucs from a single sample (not recommended unless small number of reads)\n",
    "        if nuc_count > 100000: \n",
    "            break\n",
    "        \n",
    "        nucs = ref_frame[(ref_frame.zmw == ref)]['midpoint'].values\n",
    "        if len(nucs) == 0: continue\n",
    "        if len(nucs) < 5: continue\n",
    "        lengths = ref_frame[(ref_frame.zmw == ref)]['length'].values                 \n",
    "\n",
    "        r0_sub = r0_frame[(r0_frame.zmw == ref)]\n",
    "        r1_sub = r1_frame[(r1_frame.zmw == ref)]\n",
    "        r2_sub = r2_frame[(r2_frame.zmw == ref)]\n",
    "        r3_sub = r3_frame[(r3_frame.zmw == ref)]\n",
    "        r4_sub = r4_frame[(r4_frame.zmw == ref)]\n",
    "\n",
    "        dist_r0 = np.subtract.outer(r0_sub['midpoint'].values, nucs)\n",
    "        dist_r0[~((dist_r0 < 100) & (dist_r0 > -100))] = np.nan\n",
    "        mlen_r0 = np.subtract.outer(r0_sub['length'].values, np.zeros(len(nucs)))\n",
    "        mlen_r0[~((dist_r0 < 100) & (dist_r0 > -100))] = np.nan    \n",
    "\n",
    "        dist_r1 = np.subtract.outer(r1_sub['midpoint'].values, nucs)\n",
    "        dist_r1[~((dist_r1 < 100) & (dist_r1 > -100))] = np.nan\n",
    "        mlen_r1 = np.subtract.outer(r1_sub['length'].values, np.zeros(len(nucs)))\n",
    "        mlen_r1[~((dist_r1 < 100) & (dist_r1 > -100))] = np.nan\n",
    "\n",
    "        dist_r2 = np.subtract.outer(r2_sub['midpoint'].values, nucs)\n",
    "        dist_r2[~((dist_r2 < 100) & (dist_r2 > -100))] = np.nan\n",
    "        mlen_r2 = np.subtract.outer(r2_sub['length'].values, np.zeros(len(nucs)))\n",
    "        mlen_r2[~((dist_r2 < 100) & (dist_r2 > -100))] = np.nan\n",
    "\n",
    "        dist_r3 = np.subtract.outer(r3_sub['midpoint'].values, nucs)\n",
    "        dist_r3[~((dist_r3 < 100) & (dist_r3 > -100))] = np.nan\n",
    "        mlen_r3 = np.subtract.outer(r3_sub['length'].values, np.zeros(len(nucs)))\n",
    "        mlen_r3[~((dist_r3 < 100) & (dist_r3 > -100))] = np.nan\n",
    "\n",
    "        dist_r4 = np.subtract.outer(r4_sub['midpoint'].values, nucs)\n",
    "        dist_r4[~((dist_r4 < 100) & (dist_r4 > -100))] = np.nan\n",
    "        mlen_r4 = np.subtract.outer(r4_sub['length'].values, np.zeros(len(nucs)))\n",
    "        mlen_r4[~((dist_r4 < 100) & (dist_r4 > -100))] = np.nan\n",
    "\n",
    "        for j in range(len(nucs)):\n",
    "\n",
    "            midpoint = nucs[j]\n",
    "            size = lengths[j]\n",
    "            coord1 = int(midpoint - 100)\n",
    "            try:\n",
    "                new_key = int(i)\n",
    "                acc = r0_acc[new_key][coord1:coord1 + 200]\n",
    "            except KeyError:\n",
    "                prefix = list(r0_acc.keys())[0].split('/')[0]\n",
    "                suffix = 'ccs'\n",
    "                new_key = \"%s/%s/%s\" % (prefix,int(i),suffix)\n",
    "                acc = r0_acc[new_key][coord1: coord1 + 200]\n",
    "            if len(acc) < 200: continue\n",
    "            zmw_r0 = acc\n",
    "            zmw_r1 = r1_acc[new_key][coord1:coord1 + 200]\n",
    "            zmw_r2 = r2_acc[new_key][coord1:coord1 + 200]\n",
    "            zmw_r3 = r3_acc[new_key][coord1:coord1 + 200]\n",
    "            zmw_r4 = r4_acc[new_key][coord1:coord1 + 200]\n",
    "\n",
    "            r0_mat.append(zmw_r0)\n",
    "            r1_mat.append(zmw_r1)\n",
    "            r2_mat.append(zmw_r2)\n",
    "            r3_mat.append(zmw_r3)\n",
    "            r4_mat.append(zmw_r4)\n",
    "\n",
    "            nuc_hist_r0 = bh.Histogram(bh.axis.Regular(40,20,200),bh.axis.Regular(40,-100,100))\n",
    "            nuc_hist_r1 = bh.Histogram(bh.axis.Regular(40,20,200),bh.axis.Regular(40,-100,100))\n",
    "            nuc_hist_r2 = bh.Histogram(bh.axis.Regular(40,20,200),bh.axis.Regular(40,-100,100))\n",
    "            nuc_hist_r3 = bh.Histogram(bh.axis.Regular(40,20,200),bh.axis.Regular(40,-100,100))\n",
    "            nuc_hist_r4 = bh.Histogram(bh.axis.Regular(40,20,200),bh.axis.Regular(40,-100,100))\n",
    "\n",
    "            nuc_hist_r0.fill(mlen_r0[:,j],dist_r0[:,j])\n",
    "            nuc_hist_r1.fill(mlen_r1[:,j],dist_r1[:,j])\n",
    "            nuc_hist_r2.fill(mlen_r2[:,j],dist_r2[:,j])\n",
    "            nuc_hist_r3.fill(mlen_r3[:,j],dist_r3[:,j])\n",
    "            nuc_hist_r4.fill(mlen_r4[:,j],dist_r4[:,j])\n",
    "\n",
    "            vals_r0 = nuc_hist_r0.values().flatten()\n",
    "            vals_r1 = nuc_hist_r1.values().flatten()\n",
    "            vals_r2 = nuc_hist_r2.values().flatten()\n",
    "            vals_r3 = nuc_hist_r3.values().flatten()\n",
    "            vals_r4 = nuc_hist_r4.values().flatten()\n",
    "\n",
    "            nuc_profiles_r0.append(vals_r0)\n",
    "            nuc_profiles_r1.append(vals_r1)\n",
    "            nuc_profiles_r2.append(vals_r2)\n",
    "            nuc_profiles_r3.append(vals_r3)\n",
    "            nuc_profiles_r4.append(vals_r4)\n",
    "\n",
    "            zmw_ids.append(i)\n",
    "            nuc_ids.append(nucs[j])\n",
    "            nuc_sizes.append(lengths[j])\n",
    "            \n",
    "            nuc_count += 1\n",
    "\n",
    "    np.save('%s_%s_individual_nucs_r0.npy' % (label_out, label) , np.vstack(nuc_profiles_r0))\n",
    "    np.save('%s_%s_individual_nucs_r1.npy' % (label_out, label) , np.vstack(nuc_profiles_r1))\n",
    "    np.save('%s_%s_individual_nucs_r2.npy' % (label_out, label) , np.vstack(nuc_profiles_r2))\n",
    "    np.save('%s_%s_individual_nucs_r3.npy' % (label_out, label) , np.vstack(nuc_profiles_r3))\n",
    "    np.save('%s_%s_individual_nucs_r4.npy' % (label_out, label) , np.vstack(nuc_profiles_r4))\n",
    "\n",
    "    np.save('%s_%s_acc_r0.npy' % (label_out, label), np.vstack(r0_mat))\n",
    "    np.save('%s_%s_acc_r1.npy' % (label_out, label), np.vstack(r1_mat))\n",
    "    np.save('%s_%s_acc_r2.npy' % (label_out, label), np.vstack(r2_mat))\n",
    "    np.save('%s_%s_acc_r3.npy' % (label_out, label), np.vstack(r3_mat))\n",
    "    np.save('%s_%s_acc_r4.npy' % (label_out, label), np.vstack(r4_mat))\n",
    "\n",
    "    nuc_profiles_r0 = []\n",
    "    nuc_profiles_r1 = []\n",
    "    nuc_profiles_r2 = []\n",
    "    nuc_profiles_r3 = []\n",
    "    nuc_profiles_r4 = []\n",
    "\n",
    "    fho = open('%s_%s_individual_labels.txt' % (label_out, label), 'w')\n",
    "\n",
    "    for i in range(len(zmw_ids)):\n",
    "        print(\"%s\\t%s\\t%s\" % (zmw_ids[i], nuc_ids[i], nuc_sizes[i]), file = fho)\n",
    "\n",
    "    fho.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46440ae-6e2e-4e67-9a94-edd26b705392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: prep sampled nucleosomes for clustering\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scanpy as sc \n",
    "from tqdm import tqdm\n",
    "from datetime import datetime \n",
    "\n",
    "# Set up paths\n",
    "ref_csv = pd.read_csv('/home/hrichter/IDLI_revisions/forZenodo/IDLI_Zenodo_SampleSheet.csv', sep=',')\n",
    "indices = np.array([0, 1])\n",
    "ref_sub = ref_csv[ref_csv.INDEX.isin(indices)]\n",
    "\n",
    "# Specify the number of nucleosomes to process per sample\n",
    "nucleosomes_per_sample = 100000\n",
    "\n",
    "# Storage lists for data & metadata\n",
    "tot_try = [] \n",
    "tot_rplot = [] \n",
    "tot_maximum = [] \n",
    "tot_lens = [] \n",
    "metadata_list = []\n",
    "\n",
    "print(\"Starting data loading and aggregation...\")\n",
    "for idx, line in tqdm(ref_sub.iterrows(), total=len(ref_sub)):\n",
    "    sample = line['SAMPLE']\n",
    "    peaks = \"random_nucs\"\n",
    "\n",
    "    dir = '/home/hrichter/IDLI_revisions/forZenodo/random_nucs/'\n",
    "    metadata_file = f\"{dir}{sample}_{peaks}_individual_labels.txt\"  \n",
    "\n",
    "    try:\n",
    "        # Load accessibility data (r1, r2, r3) - 140bp\n",
    "        b_full = np.load(f\"{dir}{sample}_{peaks}_acc_r1.npy\")[:, 30:170]\n",
    "        c_full = np.load(f\"{dir}{sample}_{peaks}_acc_r2.npy\")[:, 30:170]\n",
    "        d_full = np.load(f\"{dir}{sample}_{peaks}_acc_r3.npy\")[:, 30:170]\n",
    "        \n",
    "        # Load individual nucleosome maximums\n",
    "        maximum_r1 = np.load(f\"{dir}{sample}_{peaks}_individual_nucs_r1.npy\")\n",
    "        maximum_r2 = np.load(f\"{dir}{sample}_{peaks}_individual_nucs_r2.npy\") \n",
    "        maximum_r3 = np.load(f\"{dir}{sample}_{peaks}_individual_nucs_r3.npy\") \n",
    "        maximum_summed = maximum_r1 + maximum_r2 + maximum_r3 \n",
    "\n",
    "        # Load nucleosome lengths (for tot_lens)\n",
    "        lengths = pd.read_csv(metadata_file, sep='\\t', header=None)[2].values # Column 2 is length\n",
    "        \n",
    "        # Load metadata\n",
    "        meta_df = pd.read_csv(metadata_file, sep=\"\\t\", header=None, names=[\"zmw\", \"midpoint\", \"length\", \"label\"])\n",
    "        \n",
    "        # Ensure metadata aligns with data\n",
    "        if len(meta_df) != b_full.shape[0]:\n",
    "            print(f\"Warning: Metadata length mismatch for {sample}. Skipping.\")\n",
    "            continue  \n",
    "\n",
    "        # Limit the number of nucleosomes (for alignment)\n",
    "        num_nucleosomes = min(nucleosomes_per_sample, b_full.shape[0])\n",
    "        \n",
    "        b_full = b_full[:num_nucleosomes]\n",
    "        c_full = c_full[:num_nucleosomes]\n",
    "        d_full = d_full[:num_nucleosomes]\n",
    "        maximum_summed = maximum_summed[:num_nucleosomes] \n",
    "        lengths = lengths[:num_nucleosomes] \n",
    "        meta_df = meta_df.iloc[:num_nucleosomes]\n",
    "\n",
    "        # Stack accessibility data for clustering (b[:, 10:-10], c[:, 10:-10], d[:, 10:-10])\n",
    "        try_cluster = np.hstack([b_full[:, 10:-10], c_full[:, 10:-10], d_full[:, 10:-10]])\n",
    "\n",
    "        # Stack accessibility data for rplot (b, c, d)\n",
    "        try_rplot = np.hstack([b_full, c_full, d_full]) \n",
    "\n",
    "        # Store data & metadata\n",
    "        tot_try.append(try_cluster)\n",
    "        tot_rplot.append(try_rplot) \n",
    "        tot_maximum.append(maximum_summed) \n",
    "        tot_lens.append(lengths) \n",
    "        \n",
    "        meta_df[\"sample\"] = sample\n",
    "        meta_df[\"peak_set\"] = peaks\n",
    "        metadata_list.append(meta_df)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Missing files for {sample}, skipping.\")\n",
    "        continue\n",
    "\n",
    "# Final matrices\n",
    "matrix_cluster = np.vstack(tot_try)\n",
    "matrix_rplot = np.vstack(tot_rplot)\n",
    "matrix_maximum = np.vstack(tot_maximum)\n",
    "array_lens = np.concatenate(tot_lens)\n",
    "\n",
    "metadata_df = pd.concat(metadata_list, ignore_index=True)\n",
    "\n",
    "# Define file_name for saving\n",
    "base_file_name = \"forZenodo_randomnucs\"\n",
    "\n",
    "# Define the working directory\n",
    "wd = \"/home/hrichter/IDLI_revisions/forZenodo/random_nucs/\"\n",
    "os.makedirs(wd, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "np.save(wd + f'{base_file_name}_rplot.npy', matrix_rplot)\n",
    "np.save(wd + f'{base_file_name}_maximum.npy', matrix_maximum)\n",
    "np.save(wd + f'{base_file_name}_tot_lens.npy', array_lens)\n",
    "print(\"\\nSuccessfully saved aligned plot data (rplot, maximum, tot_lens).\")\n",
    "\n",
    "# Convert to AnnData object, and save\n",
    "mat = sc.AnnData(X=np.nan_to_num(matrix_cluster))\n",
    "mat.obs = metadata_df\n",
    "mat.write(wd + f\"{base_file_name}_nucs.h5ad\")\n",
    "\n",
    "print(\"AnnData object successfully created with accessibility data and metadata for specified samples and nucleosomes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b841a-f067-4730-9daa-ad592a80b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional: plot summed horizon plot for all nucs processed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import cm\n",
    "\n",
    "summed = np.sum(matrix_maximum,axis=0)\n",
    "reshaped = summed.reshape(40,40)\n",
    "\n",
    "imshow(sp.stats.zscore(reshaped[::-1], axis=1),cmap=cm.Spectral_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720e4c2-1ab4-4303-906e-74ab9a7c0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3: do the clustering, using the prior AnnData object\n",
    "\n",
    "wd = \"/home/hrichter/IDLI_revisions/forZenodo/random_nucs/\"\n",
    "base_file_name = \"forZenodo_randomnucs\"\n",
    "res_str = '50' #what you want your clustering resolution to be saved as in the file name\n",
    "\n",
    "mat = sc.read_h5ad(wd + f\"{base_file_name}_nucs.h5ad\")\n",
    "\n",
    "# Function to cluster the data and store cluster metadata + parameters\n",
    "def cluster_mats(mat, res=0.6, neighbors=15):\n",
    "    \"\"\"Clusters an AnnData object using Leiden clustering and adds cluster labels to metadata.\"\"\"\n",
    "    sc.tl.pca(mat)  \n",
    "    sc.pp.neighbors(mat, metric='correlation', n_neighbors=neighbors)  \n",
    "    sc.tl.leiden(mat, resolution=res)  \n",
    "\n",
    "    mat.obs['cluster'] = mat.obs['leiden'].astype(int)  # Convert cluster labels to integers\n",
    "\n",
    "    # Store clustering parameters in `uns`\n",
    "    clustering_params = {\n",
    "        \"neighbors\": neighbors,\n",
    "        \"resolution\": res,\n",
    "        \"clustering_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    mat.uns[\"clustering\"] = clustering_params\n",
    "\n",
    "    return mat  \n",
    "\n",
    "# Perform clustering and update metadata\n",
    "mat = cluster_mats(mat, res=0.5, neighbors=15) #can change resolution/neighbors here\n",
    "mat.write(wd + f\"{base_file_name}_nucs.h5ad\")\n",
    "\n",
    "# Save cluster assignments separately for downstream file-making\n",
    "clusters = mat.obs['cluster'].values\n",
    "np.save(wd + f\"try_cluster_res_{res_str}_{base_file_name}.npy\", clusters)\n",
    "\n",
    "print(\"Clustering complete. Unique clusters:\", np.unique(clusters))\n",
    "print(\"Stored clustering parameters:\", mat.uns[\"clustering\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30358e6-e0fc-4c74-b97a-3b1009a7d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4: makes files for plotting clusters in R to check cluster identities\n",
    "\n",
    "import os,sys,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import spatial\n",
    "import scipy.stats\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "\n",
    "# Setup\n",
    "base_path = '/home/hrichter/IDLI_revisions/forZenodo/random_nucs/'\n",
    "os.chdir(base_path)\n",
    "base_file_name = \"forZenodo_randomnucs\"\n",
    "res_str = '50' \n",
    "\n",
    "clusters = np.load(base_path + f'try_cluster_res_{res_str}_{base_file_name}.npy', allow_pickle = True)\n",
    "\n",
    "try_rplot = np.load(base_path + f'{base_file_name}_rplot.npy', allow_pickle=True)\n",
    "maximum = np.load(base_path + f'{base_file_name}_maximum.npy', allow_pickle=True)\n",
    "tot_lens = np.load(base_path + f'{base_file_name}_tot_lens.npy', allow_pickle=True)\n",
    "\n",
    "# Initial Data Check\n",
    "print(f\"Clusters array shape: {clusters.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if not (len(clusters) == len(try_rplot) == len(maximum) == len(tot_lens)):\n",
    "    print(\"FATAL ERROR: Aligned data array lengths do not match cluster array length. Check 'step 2' code.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "target = 0.99 * len(clusters) #can adjust depending on whethe\n",
    "cusum = 0\n",
    "unique_clusters = np.unique(clusters)\n",
    "cluster_sizes = {str(c): len(clusters[clusters == c]) for c in unique_clusters} \n",
    "sorted_clusters = sorted([int(c) for c in unique_clusters]) \n",
    "\n",
    "fho = open(base_path + f'plotme_vplots_clusters_{base_file_name}_res{res_str}.txt', 'w')\n",
    "fho2 = open(base_path + f'plotme_accplots_clusters_{base_file_name}_res{res_str}.txt', 'w')\n",
    "fho3 = open(base_path + f'plotme_lengths_clusters_{base_file_name}_res{res_str}.txt', 'w')\n",
    "\n",
    "for i in sorted_clusters:\n",
    "    # i is the integer cluster ID (e.g., 0, 1, 2)\n",
    "    cluster_label_str = str(i) #\n",
    "\n",
    "    current_size = cluster_sizes.get(cluster_label_str, 0) \n",
    "\n",
    "    cusum += current_size\n",
    "    \n",
    "    print(f\"\\nProcessing cluster {i}\")\n",
    "    print(f\"Cluster size: {current_size}\")\n",
    "    \n",
    "    cluster_mask = (clusters == i)\n",
    "\n",
    "    sub = maximum[cluster_mask]\n",
    "\n",
    "    if sub.shape[0] == 0:\n",
    "        print(f\"ERROR: 'maximum' subset for cluster {i} is empty. Skipping cluster.\")\n",
    "        continue\n",
    "        \n",
    "    summed = np.sum(sub, axis=0)\n",
    "    # reshaped = summed.reshape(40, 40) # Original size 40x40 = 1600\n",
    "    try:\n",
    "        reshaped = summed.reshape(40, 40)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reshaping 'maximum' for cluster {i}: {e}. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    plotter = sp.stats.zscore(reshaped[::-1], axis=1) # Reverse rows and z-score across columns\n",
    "\n",
    "    # Print V-plot data to fho\n",
    "    for j in range(21, 200):\n",
    "        for k in range(-100, 100):\n",
    "            ci = int((200 - j) * 40 / 180)\n",
    "            cj = int((k + 100) / 5)\n",
    "            if 0 <= ci < 40 and 0 <= cj < 40:\n",
    "                print(\"%s\\t%s\\t%s\\t%s\" % (j, k, plotter[ci][cj], i + 1), file=fho)\n",
    "\n",
    "    sub_acc = try_rplot[cluster_mask]\n",
    "    \n",
    "    acc = sub_acc[:, :140]\n",
    "    acc2 = sub_acc[:, 140:280]\n",
    "    acc3 = sub_acc[:, 280:]\n",
    "    \n",
    "    acc_mean = np.nanmean(acc, axis=0)\n",
    "    acc2_mean = np.nanmean(acc2, axis=0)\n",
    "    acc3_mean = np.nanmean(acc3, axis=0)\n",
    "    \n",
    "    # Print accessibility data to fho2\n",
    "    for j in range(140):\n",
    "        x_coord = j - 70 \n",
    "        y_coord1 = acc_mean[j] if not np.isnan(acc_mean[j]) else 0\n",
    "        y_coord2 = acc2_mean[j] if not np.isnan(acc2_mean[j]) else 0\n",
    "        y_coord3 = acc3_mean[j] if not np.isnan(acc3_mean[j]) else 0\n",
    "        \n",
    "        print(\"%s\\t%s\\t%s\\t31\" % (x_coord, y_coord1, i + 1), file=fho2)\n",
    "        print(\"%s\\t%s\\t%s\\t51\" % (x_coord, y_coord2, i + 1), file=fho2)\n",
    "        print(\"%s\\t%s\\t%s\\t71\" % (x_coord, y_coord3, i + 1), file=fho2)\n",
    "\n",
    "    sublens = tot_lens[cluster_mask]\n",
    "    \n",
    "    # Print length data to fho3\n",
    "    for j in range(len(sublens)):\n",
    "        print(\"%s\\t%s\" % (sublens[j], i + 1), file=fho3)\n",
    "        \n",
    "    if cusum >= target:\n",
    "        print(f\"\\nReached {cusum/len(clusters) * 100:.2f}% coverage. Stopping after cluster {i}.\")\n",
    "        break \n",
    "\n",
    "fho.close()\n",
    "fho2.close()\n",
    "fho3.close()\n",
    "\n",
    "print(\"\\nPlot data generation complete using newly aligned arrays.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idli_revisions",
   "language": "python",
   "name": "idli_revisions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
